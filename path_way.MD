
## Module 2: Classification

- [X] [Naive Bayes classification](https://izhangzhihao.github.io/2017/11/16/朴素贝叶斯分类/)
- [X] K-NN classification (Skipped)
- [X] [Linear Discriminant Analysis](https://izhangzhihao.github.io/2017/11/17/主成分分析(PCA)和线性判别分析(LDA)/)

## Module 3: Regression

- [ ] Linear Regression
- [ ] Logistic Regression
- [ ] Nonlinear Regression / Polynomial Regression

## Module 4: Unsupervised learning

- [ ] Principal Component Analysis
- [ ] Euclidean distance, edit distance, jaccard distance
- [ ] Cosine similarity
- [ ] Clustering
    - [ ] K-means clustering
    - [ ] Hierarchical clustering
    - [ ] Spectral clustering
    - [ ] GMM clustering

## Module 5: Machine Learning Model Development (intermediate - advanced)

- [ ] Evaluating machine learning models
    - [ ] Curse of dimensionality / Occam’s razor
        - [ ] Vapnik-Chernovenkis (VC) dimension
    - [ ] Underfitting and overfitting
    - [ ] Hypothesis testing and Statistical significance
    - [ ] Cross validation
        - [ ] Leave one-out
        - [ ] K-fold validation
    - [ ] Bootstrap
- [ ] Training error and loss functions
    - [ ] Absolute loss
    - [ ] Squared loss
    - [ ] Zero/one loss
    - [ ] Cross entropy loss
- [ ] Advanced Feature Engineering
- [ ] Algorithm Selection

## Module 6: Decision Trees 

- [ ] Features
- [ ] Loss function

## Module 7: Ensemble Learning

- [ ] Bagging
- [ ] Boosting
- [ ] Random forest

## Module 8: Support Vector Machines

- [ ] Linear SVM
- [ ] ELM - Extreme Learning Machine

## Module 9: Perceptrons

- [ ] Perceptrons
- [ ] Multi-layer perceptrons
- [ ] Momentum

## Module 10: Neural Networks

- [ ] Back-propagation networks
- [ ] Multi-layer networks
- [ ] Hopfield networks
- [ ] Softmax networks

## Module 11: Expectation-Maximisation

- [ ] Expectation-maximisation (EM) algorithms

## Module 12: Graphical Models

- [ ] Bayesian networks
- [ ] Markov models
- [ ] Markov random fields
- [ ] Hidden Markov models
- [ ] Conditional random field